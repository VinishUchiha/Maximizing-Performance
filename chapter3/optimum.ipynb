{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# distilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!optimum-cli export onnx --model distilbert-base-uncased-finetuned-sst-2-english --optimize O4 distilbert_onnx/ --device cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "import torch\n",
    "import logging\n",
    "logging.disable(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-16 18:25:37.881117089 [W:onnxruntime:, session_state.cc:1136 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\n",
      "2023-05-16 18:25:37.881128100 [W:onnxruntime:, session_state.cc:1138 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')\n",
    "onnx_model = ORTModelForSequenceClassification.from_pretrained('distilbert_onnx', provider='CUDAExecutionProvider')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "_ = model.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "sentences = [\n",
    "    \"Mars is the fourth planet from the Sun.\",\n",
    "    \"has a crust primarily composed of elements\",\n",
    "    \"However, it is unknown\",\n",
    "    \"can be viewed from Earth\",\n",
    "    \"It was the Romans\",\n",
    "]\n",
    "\n",
    "len_dataset = 100\n",
    "\n",
    "texts = []\n",
    "for _ in range(len_dataset):\n",
    "    n_times = random.randint(1, 30)\n",
    "    texts.append(\" \".join(random.choice(sentences) for _ in range(n_times)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average response time for original distilBERT: 0.003664400577545166 ms\n"
     ]
    }
   ],
   "source": [
    "classification = pipeline(\"text-classification\", model=model, tokenizer=tokenizer, device=\"cuda:0\")\n",
    "start = time.time()\n",
    "classification(texts)\n",
    "end = time.time()\n",
    "print(f\"Average response time for original distilBERT: {(end-start)/100} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average response time for original distilBERT: 0.0016323709487915039 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-16 18:25:46.483073433 [W:onnxruntime:, session_state.cc:1136 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\n",
      "2023-05-16 18:25:46.483083402 [W:onnxruntime:, session_state.cc:1138 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\n"
     ]
    }
   ],
   "source": [
    "onnx_classification = pipeline(\"text-classification\", model=onnx_model, tokenizer=tokenizer, device=\"cuda:0\")\n",
    "start = time.time()\n",
    "onnx_classification(texts)\n",
    "end = time.time()\n",
    "print(f\"Average response time for original distilBERT: {(end-start)/100} ms\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!optimum-cli export onnx --model gpt2 --optimize O4 gpt2_onnx/ --device cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, pipeline, AutoModelForCausalLM\n",
    "from optimum.onnxruntime import ORTModelForCausalLM\n",
    "import logging\n",
    "logging.disable(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-16 18:25:56.538311185 [W:onnxruntime:, session_state.cc:1136 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\n",
      "2023-05-16 18:25:56.538323819 [W:onnxruntime:, session_state.cc:1138 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "model = AutoModelForCausalLM.from_pretrained('gpt2')\n",
    "onnx_model = ORTModelForCausalLM.from_pretrained('gpt2_onnx', provider='CUDAExecutionProvider')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "_ = model.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average response time for original GPT2: 0.03029601812362671 ms\n"
     ]
    }
   ],
   "source": [
    "generation = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=\"cuda:0\")\n",
    "start = time.time()\n",
    "generation(texts)\n",
    "end = time.time()\n",
    "print(f\"Average response time for original GPT2: {(end-start)/100} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-16 18:26:15.795406358 [W:onnxruntime:, session_state.cc:1136 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\n",
      "2023-05-16 18:26:15.795419262 [W:onnxruntime:, session_state.cc:1138 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average response time for onnx GPT2: 0.022464430332183837 ms\n"
     ]
    }
   ],
   "source": [
    "onnx_generation = pipeline(\"text-generation\", model=onnx_model, tokenizer=tokenizer, device=\"cuda:0\")\n",
    "start = time.time()\n",
    "onnx_generation(texts)\n",
    "end = time.time()\n",
    "print(f\"Average response time for onnx GPT2: {(end-start)/100} ms\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!optimum-cli export onnx --model t5-small --optimize O4 t5_onnx/ --device cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "from optimum.onnxruntime import ORTModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained('t5-small')\n",
    "onnx_model = ORTModelForSeq2SeqLM.from_pretrained(\"t5_onnx\", use_io_binding=True, provider='CUDAExecutionProvider')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "_ = model.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average response time for original T5: 0.22183520793914796 ms\n"
     ]
    }
   ],
   "source": [
    "summarization = pipeline(\"summarization\", model=model, tokenizer=tokenizer, device=\"cuda:0\")\n",
    "start = time.time()\n",
    "summarization(texts)\n",
    "end = time.time()\n",
    "print(f\"Average response time for original T5: {(end-start)/100} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average response time for onnx T5: 0.1483987784385681 ms\n"
     ]
    }
   ],
   "source": [
    "onnx_summarization = pipeline(\"summarization\", model=onnx_model, tokenizer=tokenizer, device=\"cuda:0\")\n",
    "start = time.time()\n",
    "onnx_summarization(texts)\n",
    "end = time.time()\n",
    "print(f\"Average response time for onnx T5: {(end-start)/100} ms\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speedster",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
