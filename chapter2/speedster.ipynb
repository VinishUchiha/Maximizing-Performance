{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased', torchscript=True)\n",
    "\n",
    "# Move the model to gpu if available and set eval mode\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "_ = model.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "sentences = [\n",
    "    \"Mars is the fourth planet from the Sun.\",\n",
    "    \"has a crust primarily composed of elements\",\n",
    "    \"However, it is unknown\",\n",
    "    \"can be viewed from Earth\",\n",
    "    \"It was the Romans\",\n",
    "]\n",
    "\n",
    "len_dataset = 100\n",
    "\n",
    "texts = []\n",
    "for _ in range(len_dataset):\n",
    "    n_times = random.randint(1, 30)\n",
    "    texts.append(\" \".join(random.choice(sentences) for _ in range(n_times)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from speedster import optimize_model, save_model, load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_info = {\n",
    "    \"inputs\": [\n",
    "        {0: 'batch', 1: 'num_tokens'},\n",
    "        {0: 'batch', 1: 'num_tokens'},\n",
    "        {0: 'batch', 1: 'num_tokens'},\n",
    "    ],\n",
    "    \"outputs\": [\n",
    "        {0: 'batch', 1: 'num_tokens'},\n",
    "        {0: 'batch'},\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Move inputs to gpu if available\n",
    "encoded_inputs = [tokenizer(text, return_tensors=\"pt\").to(device) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_model = optimize_model(\n",
    "    model=model,\n",
    "    input_data=encoded_inputs,\n",
    "    optimization_time=\"constrained\",\n",
    "    ignore_compilers=[\"tensor_rt\", \"tvm\"],  # TensorRT does not work for this model\n",
    "    dynamic_info=dynamic_info,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average response time for original BERT: 5.4140496253967285 ms\n"
     ]
    }
   ],
   "source": [
    "times = []\n",
    "\n",
    "# Warmup for 30 iterations\n",
    "for encoded_input in encoded_inputs[:30]:\n",
    "    with torch.no_grad():\n",
    "        final_out = model(**encoded_input)\n",
    "\n",
    "# Benchmark\n",
    "for encoded_input in encoded_inputs:\n",
    "    st = time.time()\n",
    "    with torch.no_grad():\n",
    "        final_out = model(**encoded_input)\n",
    "    times.append(time.time()-st)\n",
    "original_model_time = sum(times)/len(times)*1000\n",
    "print(f\"Average response time for original BERT: {original_model_time} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average response time for optimized BERT (no metric drop): 4.281759262084961 ms\n"
     ]
    }
   ],
   "source": [
    "times = []\n",
    "\n",
    "# Warmup for 30 iterations\n",
    "for encoded_input in encoded_inputs[:30]:\n",
    "    with torch.no_grad():\n",
    "        final_out = optimized_model(**encoded_input)\n",
    "\n",
    "# Benchmark\n",
    "for encoded_input in encoded_inputs:\n",
    "    st = time.time()\n",
    "    with torch.no_grad():\n",
    "        final_out = optimized_model(**encoded_input)\n",
    "    times.append(time.time()-st)\n",
    "optimized_model_time = sum(times)/len(times)*1000\n",
    "print(f\"Average response time for optimized BERT (no metric drop): {optimized_model_time} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_model = optimize_model(\n",
    "    model=model,\n",
    "    input_data=encoded_inputs,\n",
    "    optimization_time=\"constrained\",\n",
    "    ignore_compilers=[\"tensor_rt\", \"tvm\"],\n",
    "    dynamic_info=dynamic_info,\n",
    "    metric_drop_ths=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average response time for optimized BERT (metric drop): 2.9406261444091797 ms\n"
     ]
    }
   ],
   "source": [
    "times = []\n",
    "\n",
    "# Warmup for 30 iterations\n",
    "for encoded_input in encoded_inputs[:30]:\n",
    "    with torch.no_grad():\n",
    "        final_out = optimized_model(**encoded_input)\n",
    "\n",
    "# Benchmark\n",
    "for encoded_input in encoded_inputs:\n",
    "    st = time.time()\n",
    "    with torch.no_grad():\n",
    "        final_out = optimized_model(**encoded_input)\n",
    "    times.append(time.time()-st)\n",
    "optimized_model_time = sum(times)/len(times)*1000\n",
    "print(f\"Average response time for optimized BERT (metric drop): {optimized_model_time} ms\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2Model.from_pretrained('gpt2', torchscript=True)\n",
    "\n",
    "# Move the model to gpu if available and set eval mode\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "_ = model.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_info = {\n",
    "    \"inputs\": [\n",
    "        {0: 'batch', 1: 'num_tokens'},\n",
    "        {0: 'batch', 1: 'num_tokens'}\n",
    "    ],\n",
    "    \"outputs\": [\n",
    "        {0: 'batch', 1: 'num_tokens'},\n",
    "    ] + [{0: 'batch', 2: 'num_tokens'} for i in range(24)]\n",
    "}\n",
    "\n",
    "optimized_model = optimize_model(\n",
    "    model=model,\n",
    "    input_data=encoded_inputs,\n",
    "    optimization_time=\"constrained\",\n",
    "    ignore_compilers=[\"tensor_rt\", \"tvm\"],\n",
    "    dynamic_info=dynamic_info,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average response time for original GPT2: 6.764638423919678 ms\n"
     ]
    }
   ],
   "source": [
    "times = []\n",
    "\n",
    "# Warmup for 30 iterations\n",
    "for encoded_input in encoded_inputs[:30]:\n",
    "    with torch.no_grad():\n",
    "        final_out = model(**encoded_input)\n",
    "\n",
    "# Benchmark\n",
    "for encoded_input in encoded_inputs:\n",
    "    st = time.time()\n",
    "    with torch.no_grad():\n",
    "        final_out = model(**encoded_input)\n",
    "    times.append(time.time()-st)\n",
    "original_model_time = sum(times)/len(times)*1000\n",
    "print(f\"Average response time for original GPT2: {original_model_time} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average response time for optimized GPT2 (no metric drop): 4.888675212860107 ms\n"
     ]
    }
   ],
   "source": [
    "times = []\n",
    "\n",
    "# Warmup for 30 iterations\n",
    "for encoded_input in encoded_inputs[:30]:\n",
    "    with torch.no_grad():\n",
    "        final_out = optimized_model(**encoded_input)\n",
    "\n",
    "# Benchmark\n",
    "for encoded_input in encoded_inputs:\n",
    "    st = time.time()\n",
    "    with torch.no_grad():\n",
    "        final_out = optimized_model(**encoded_input)\n",
    "    times.append(time.time()-st)\n",
    "optimized_model_time = sum(times)/len(times)*1000\n",
    "print(f\"Average response time for optimized GPT2 (no metric drop): {optimized_model_time} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_model = optimize_model(\n",
    "    model=model,\n",
    "    input_data=encoded_inputs,\n",
    "    optimization_time=\"constrained\",\n",
    "    ignore_compilers=[\"tensor_rt\", \"tvm\"],\n",
    "    dynamic_info=dynamic_info,\n",
    "    metric_drop_ths=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average response time for optimized GPT2 (metric drop): 3.5162806510925293 ms\n"
     ]
    }
   ],
   "source": [
    "times = []\n",
    "\n",
    "# Warmup for 30 iterations\n",
    "for encoded_input in encoded_inputs[:30]:\n",
    "    with torch.no_grad():\n",
    "        final_out = optimized_model(**encoded_input)\n",
    "\n",
    "# Benchmark\n",
    "for encoded_input in encoded_inputs:\n",
    "    st = time.time()\n",
    "    with torch.no_grad():\n",
    "        final_out = optimized_model(**encoded_input)\n",
    "    times.append(time.time()-st)\n",
    "optimized_model_time = sum(times)/len(times)*1000\n",
    "print(f\"Average response time for optimized GPT2 (metric drop): {optimized_model_time} ms\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "import time\n",
    "\n",
    "model_name = \"google/t5-efficient-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torchscript=True)\n",
    "\n",
    "# Move the model to gpu if available and set eval mode\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "_ = model.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = model.get_encoder()\n",
    "decoder = model.get_decoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"\"\"BERT is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labeling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts.\"\"\",\n",
    "    \"\"\"GPT-2 is a transformers model pretrained on a very large corpus of English data in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it was trained to guess the next word in sentences.\"\"\",\n",
    "    \"\"\"With T5, we propose reframing all NLP tasks into a unified text-to-text-format where the input and output are always text strings, in contrast to BERT-style models that can only output either a class label or a span of the input. Our text-to-text framework allows us to use the same model, loss function, and hyperparameters on any NLP task.\"\"\",\n",
    "    \"\"\"LayoutLMv3 is a pre-trained multimodal Transformer for Document AI with unified text and image masking. The simple unified architecture and training objectives make LayoutLMv3 a general-purpose pre-trained model. For example, LayoutLMv3 can be fine-tuned for both text-centric tasks, including form understanding, receipt understanding, and document visual question answering, and image-centric tasks such as document image classification and document layout analysis.\"\"\",\n",
    "    \"\"\"XLNet is a new unsupervised language representation learning method based on a novel generalized permutation language modeling objective. Additionally, XLNet employs Transformer-XL as the backbone model, exhibiting excellent performance for language tasks involving long context. Overall, XLNet achieves state-of-the-art (SOTA) results on various downstream language tasks including question answering, natural language inference, sentiment analysis, and document ranking.\"\"\"\n",
    "]\n",
    "texts = texts*20\n",
    "encoded_inputs = [tokenizer(text, padding=\"longest\", return_tensors=\"pt\").to(device) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_dynamic_info = {\n",
    "    \"inputs\": [\n",
    "        {0: 'batch', 1: 'num_tokens'},\n",
    "        {0: 'batch', 1: 'num_tokens'}\n",
    "    ],\n",
    "    \"outputs\": [\n",
    "        {0: 'batch', 1: 'num_tokens'},\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create the optimized encoder model seperately\n",
    "optimized_encoder_model = optimize_model(\n",
    "    model=encoder,\n",
    "    input_data=encoded_inputs,\n",
    "    optimization_time=\"constrained\",\n",
    "    ignore_compilers=[\"tensor_rt\", \"tvm\"],\n",
    "    dynamic_info=encoder_dynamic_info,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_dynamic_info = {\n",
    "    \"inputs\": [\n",
    "        {0: 'batch', 1: 'num_tokens'},\n",
    "        {0: 'batch', 1: 'num_tokens'}\n",
    "    ],\n",
    "    \"outputs\": [\n",
    "        {0: 'batch', 1: 'num_tokens'},\n",
    "    ] + [{0: 'batch', 2: 'num_tokens'} for i in range(24)]\n",
    "}\n",
    "\n",
    "# Create the optimized decoder model seperately\n",
    "optimized_decoder_model = optimize_model(\n",
    "    model=decoder,\n",
    "    input_data=encoded_inputs,\n",
    "    optimization_time=\"constrained\",\n",
    "    ignore_compilers=[\"tensor_rt\", \"tvm\"],\n",
    "    dynamic_info=decoder_dynamic_info,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = encoder.to(device)\n",
    "_ = decoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average response time for original T5: 14.36943769454956 ms\n"
     ]
    }
   ],
   "source": [
    "times = []\n",
    "# Warmup for 30 iterations\n",
    "for encoded_input in encoded_inputs[:30]:\n",
    "    with torch.no_grad():\n",
    "        encoder_out = encoder(**encoded_input)\n",
    "        decoder_out = decoder(**encoded_input,encoder_hidden_states=encoder_out[0])\n",
    "\n",
    "# Benchmark\n",
    "for encoded_input in encoded_inputs:\n",
    "    st = time.time()\n",
    "    with torch.no_grad():\n",
    "        encoder_out = encoder(**encoded_input)\n",
    "        decoder_out = decoder(**encoded_input,encoder_hidden_states=encoder_out[0])\n",
    "    times.append(time.time()-st)\n",
    "original_model_time = sum(times)/len(times)*1000\n",
    "print(f\"Average response time for original T5: {original_model_time} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average response time for optimized T5 (no metric drop): 9.04134750366211 ms\n"
     ]
    }
   ],
   "source": [
    "times = []\n",
    "\n",
    "# Warmup for 30 iterations\n",
    "for encoded_input in encoded_inputs[:30]:\n",
    "    with torch.no_grad():\n",
    "        encoder_out = optimized_encoder_model(**encoded_input)\n",
    "        decoder_out = optimized_decoder_model(**encoded_input,encoder_hidden_states=encoder_out[0])\n",
    "\n",
    "# Benchmark\n",
    "for encoded_input in encoded_inputs:\n",
    "    st = time.time()\n",
    "    with torch.no_grad():\n",
    "        encoder_out = optimized_encoder_model(**encoded_input)\n",
    "        decoder_out = optimized_decoder_model(**encoded_input,encoder_hidden_states=encoder_out[0])\n",
    "    times.append(time.time()-st)\n",
    "optimized_model_time = sum(times)/len(times)*1000\n",
    "print(f\"Average response time for optimized T5 (no metric drop): {optimized_model_time} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_encoder_model = optimize_model(\n",
    "    model=encoder,\n",
    "    input_data=encoded_inputs,\n",
    "    optimization_time=\"constrained\",\n",
    "    ignore_compilers=[\"tensor_rt\", \"tvm\"],\n",
    "    dynamic_info=encoder_dynamic_info,\n",
    "    metric_drop_ths=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_decoder_model = optimize_model(\n",
    "    model=decoder,\n",
    "    input_data=encoded_inputs,\n",
    "    optimization_time=\"constrained\",\n",
    "    ignore_compilers=[\"tensor_rt\", \"tvm\"],\n",
    "    dynamic_info=decoder_dynamic_info,\n",
    "    metric_drop_ths=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = encoder.to(device)\n",
    "_ = decoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average response time for optimized T5 (metric drop): 7.490894794464111 ms\n"
     ]
    }
   ],
   "source": [
    "times = []\n",
    "\n",
    "# Warmup for 30 iterations\n",
    "for encoded_input in encoded_inputs[:30]:\n",
    "    with torch.no_grad():\n",
    "        encoder_out = optimized_encoder_model(**encoded_input)\n",
    "        decoder_out = optimized_decoder_model(**encoded_input,encoder_hidden_states=encoder_out[0])\n",
    "\n",
    "# Benchmark\n",
    "for encoded_input in encoded_inputs:\n",
    "    st = time.time()\n",
    "    with torch.no_grad():\n",
    "        encoder_out = optimized_encoder_model(**encoded_input)\n",
    "        decoder_out = optimized_decoder_model(**encoded_input,encoder_hidden_states=encoder_out[0])\n",
    "    times.append(time.time()-st)\n",
    "optimized_model_time = sum(times)/len(times)*1000\n",
    "print(f\"Average response time for optimized T5 (metric drop): {optimized_model_time} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speedster",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
